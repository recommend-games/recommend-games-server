""" utils """

import json
import logging
import os.path
import re
import timeit
from csv import DictWriter
from datetime import datetime, timezone
from functools import lru_cache, partial
from pathlib import Path
from typing import Iterable, Optional, Union
import uuid

from django.conf import settings
from pytility import arg_to_iter, normalize_space, parse_date

LOGGER = logging.getLogger(__name__)
VERSION_REGEX = re.compile(r"^\D*(.+)$")


def format_from_path(path):
    """get file extension"""
    try:
        _, ext = os.path.splitext(path)
        return ext.lower()[1:] if ext else None
    except Exception:
        pass
    return None


def serialize_date(date, tzinfo=None):
    """seralize a date into ISO format if possible"""
    parsed = parse_date(date, tzinfo)
    return parsed.strftime("%Y-%m-%dT%T%z") if parsed else str(date) if date else None


@lru_cache(maxsize=8)
def load_recommender(path, site="bgg"):
    """load recommender from given path"""

    if not path:
        return None

    try:
        if site == "light":
            from board_game_recommender import LightGamesRecommender

            LOGGER.info("Trying to load <LightGamesRecommender> from <%s>", path)
            return LightGamesRecommender.from_npz(path)

        if site == "bga":
            from board_game_recommender import BGARecommender

            LOGGER.info("Trying to load <BGARecommender> from <%s>", path)
            return BGARecommender.load(path=path)

        from board_game_recommender import BGGRecommender

        LOGGER.info("Trying to load <BGGRecommender> from <%s>", path)
        return BGGRecommender.load(path=path)

    except Exception:
        LOGGER.exception("unable to load recommender model from <%s>", path)

    return None


@lru_cache(maxsize=8)
def model_updated_at(file_path=settings.MODEL_UPDATED_FILE):
    """latest model update"""
    try:
        with open(file_path, encoding="utf-8") as file_obj:
            updated_at = file_obj.read()
        updated_at = normalize_space(updated_at)
        return parse_date(updated_at, tzinfo=timezone.utc)
    except Exception:
        pass
    return None


def parse_version(version):
    """Parse a version string to strip leading "v" etc."""
    version = normalize_space(version)
    if not version:
        return None
    match = VERSION_REGEX.match(version)
    return match.group(1) if match else None


@lru_cache(maxsize=8)
def project_version(file_path=settings.PROJECT_VERSION_FILE):
    """Project version."""
    try:
        with open(file_path, encoding="utf-8") as file_obj:
            version = file_obj.read()
        return parse_version(version)
    except Exception:
        pass
    return None


@lru_cache(maxsize=8)
def server_version(file_path=settings.PROJECT_VERSION_FILE) -> dict:
    """Full server version."""
    release_version = project_version(file_path=file_path)
    heroku_version = parse_version(os.getenv("HEROKU_RELEASE_VERSION"))
    server_version = "-".join(filter(None, (release_version, heroku_version)))
    return {
        "project_version": release_version,
        "server_version": server_version or None,
    }


def save_recommender_ranking(recommender, dst, similarity_model=False):
    """Save the rankings generated by a recommender to a CSV file."""

    LOGGER.info(
        "Saving <%s> ranking to <%s>...",
        recommender.similarity_model if similarity_model else recommender.model,
        dst,
    )

    recommendations = recommender.recommend(users=(), similarity_model=similarity_model)
    if "name" in recommendations.column_names():
        recommendations.remove_column("name", inplace=True)

    if similarity_model:
        recommendations = recommendations[recommendations["score"] > 0]

    recommendations.export_csv(str(dst))


def count_lines(path) -> int:
    """Return the line count of a given path."""
    with open(path, encoding="utf-8") as file:
        return sum(1 for _ in file)


def count_files(path, glob=None) -> int:
    """Return the number of files in a given directory."""
    path = Path(path)
    files = path.glob(glob) if glob else path.iterdir()
    return sum(1 for file in files if file.is_file())


def count_lines_and_files(
    paths_lines=None,
    paths_files=None,
    line_glob=None,
    file_glob=None,
) -> dict:
    """Counts lines and files in the given paths."""

    result = {}

    for path in arg_to_iter(paths_lines):
        path = Path(path).resolve()
        if path.is_dir():
            files = path.glob(line_glob) if line_glob else path.iterdir()
        elif path.is_file():
            files = (path,)
        else:
            files = ()
        for file in files:
            LOGGER.info("Counting lines in <%s>...", file)
            name = os.path.splitext(file.name)[0]
            result[f"lc_{name}"] = count_lines(file)

    for path in arg_to_iter(paths_files):
        path = Path(path).resolve()
        if not path.is_dir():
            continue
        for subdir in path.glob("**"):
            LOGGER.info("Counting files in <%s>...", subdir)
            if path == subdir:
                name = path.name
            else:
                relative = subdir.relative_to(path)
                name = "_".join(relative.parts)
            result[f"fc_{name}"] = count_files(subdir, glob=file_glob)

    return result


def _process_value(value, joiner=","):
    if value is None:
        return ""
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, (list, tuple)):
        return joiner.join(
            str(v).rsplit(":", maxsplit=1)[-1]
            for v in value
            if v is not None and v != ""
        )
    return value


def _process_row(row, columns=None, joiner=","):
    if isinstance(row, str):
        row = json.loads(row)
    if columns is None:
        return {key: _process_value(value, joiner=joiner) for key, value in row.items()}
    return {
        column: _process_value(row.get(column), joiner=joiner) for column in columns
    }


def jl_to_csv(in_path, out_path, columns=None, joiner=","):
    """Convert a JSON lines file into CSV."""

    columns = tuple(arg_to_iter(columns))

    LOGGER.info(
        "Reading JSON lines from <%s> and writing CSV to <%s>...", in_path, out_path
    )

    with open(in_path, encoding="utf-8") as in_file, open(
        out_path,
        "w",
        encoding="utf-8",
    ) as out_file:
        if not columns:
            row = next(in_file, None)
            row = _process_row(row, joiner=joiner) if row else {}
            columns = tuple(row.keys())
        else:
            row = None

        rows = map(partial(_process_row, columns=columns, joiner=joiner), in_file)

        writer = DictWriter(out_file, fieldnames=columns)
        writer.writeheader()
        if row:
            writer.writerow(row)
        writer.writerows(rows)


class Timer:
    """log execution time: with Timer('message'): do_something()"""

    def __init__(self, message, logger=None):
        self.message = f'"{message}" execution time: %.1f ms'
        self.logger = logger
        self.start = None

    def __enter__(self):
        self.start = timeit.default_timer()
        return self

    def __exit__(self, *args, **kwargs):
        duration = 1000 * (timeit.default_timer() - self.start)
        if self.logger is None:
            print(self.message % duration)
        else:
            self.logger.info(self.message, duration)


def gitlab_merge_request(
    *,
    file_path: str,
    file_content: str,
    gitlab_project_id: int,
    gitlab_access_token: str,
    gitlab_url: str = "https://gitlab.com",
    source_branch: Optional[str] = None,
    target_branch: str = "main",
    title: Optional[str] = None,
    description: Optional[str] = None,
) -> str:
    """Upload a file to GitLab and create a merge request."""

    try:
        import gitlab
    except ImportError:
        LOGGER.exception("Please make sure <python-gitlab> is installed")
        raise

    gl = gitlab.Gitlab(gitlab_url, private_token=gitlab_access_token)
    project = gl.projects.get(gitlab_project_id)

    # create a new branch
    source_branch = source_branch or f"mr-{uuid.uuid4()}"
    try:
        branch = project.branches.create(
            {
                "branch": source_branch,
                "ref": target_branch,
            }
        )
        LOGGER.info(
            "Created branch <%s> from commit <%s>",
            branch.name,
            branch.commit["id"],
        )
    except gitlab.exceptions.GitlabCreateError:
        LOGGER.exception(
            "Failed to create branch <%s> from <%s>",
            source_branch,
            target_branch,
        )
        raise

    # upload file
    commit_message = f"Added {file_path}"
    try:
        file = project.files.create(
            {
                "file_path": file_path,
                "branch": source_branch,
                "content": file_content,
                "commit_message": commit_message,
            }
        )
        LOGGER.info(
            "Uploaded file <%s> to <%s>",
            file_path,
            file.branch,
        )
    except gitlab.exceptions.GitlabCreateError:
        LOGGER.exception(
            "Failed to upload file <%s> to <%s>",
            file_path,
            source_branch,
        )
        raise

    # create merge request
    try:
        mr = project.mergerequests.create(
            {
                "source_branch": source_branch,
                "target_branch": target_branch,
                "title": title or commit_message,
                "description": description or commit_message,
            }
        )
        LOGGER.info(
            "Created merge request <%s>",
            mr.web_url,
        )
    except gitlab.exceptions.GitlabCreateError:
        LOGGER.exception(
            "Failed to create merge request from <%s> to <%s>",
            source_branch,
            target_branch,
        )
        raise

    return mr.web_url


def premium_feature_gitlab_merge_request(
    *,
    users: Iterable[str],
    access_expiration: Union[datetime, str],
    gitlab_project_id: int,
    gitlab_access_token: str,
    file_dir: str = "users/premium",
    file_stem: Optional[str] = None,
    gitlab_url: str = "https://gitlab.com",
    source_branch: Optional[str] = None,
    target_branch: str = "main",
    title: Optional[str] = None,
    description: Optional[str] = None,
) -> str:
    """Create a merge request to add users to the premium list."""

    try:
        import yaml
    except ImportError:
        LOGGER.exception("Please make sure <pyyaml> is installed")
        raise

    users = sorted(frozenset(user.lower() for user in arg_to_iter(users)))
    if not users:
        raise ValueError("No users provided")

    access_expiration = parse_date(access_expiration, tzinfo=timezone.utc)
    if not access_expiration:
        raise ValueError("Invalid access expiration")

    data = [{user: access_expiration.replace()} for user in users]
    data_yaml = yaml.safe_dump(data)
    now = datetime.utcnow().isoformat(timespec="seconds")
    file_content = f"# Generated at {now}Z\n{data_yaml}"
    sand = uuid.uuid4()
    file_path = f"{file_dir}/{file_stem or sand}.yaml"
    source_branch = source_branch or f"premium-{sand}"

    return gitlab_merge_request(
        file_path=file_path,
        file_content=file_content,
        gitlab_project_id=gitlab_project_id,
        gitlab_access_token=gitlab_access_token,
        gitlab_url=gitlab_url,
        source_branch=source_branch,
        target_branch=target_branch,
        title=title or f"Add {len(users)} users to premium list",
        description=description
        or (
            f"Request to add these users to the premium list:\n\n"
            + "\n".join(f"- {user}" for user in users)
        ),
    )
